{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First, we allow Google Collab access to our drive so we can access the Kaggle data."
      ],
      "metadata": {
        "id": "yh5YDaERmEbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMB-CIpNkpx6",
        "outputId": "3eef7bce-c3cd-478d-e90e-032b3ed04084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use the provided code to actually load in our data."
      ],
      "metadata": {
        "id": "crjmj0VBmMEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_npz(file_path):\n",
        "    with np.load(file_path) as data:\n",
        "        return {key: data[key] for key in data}\n",
        "\n",
        "train_data = load_npz('/content/drive/MyDrive/movie-review-preference-analysis/train.npz')\n",
        "test_data = load_npz('/content/drive/MyDrive/movie-review-preference-analysis/test.npz')\n",
        "train_emb1, train_emb2, train_labels = train_data['emb1'], train_data['emb2'], train_data['preference']\n",
        "test_emb1, test_emb2 = test_data['emb1'], test_data['emb2']\n",
        "\n",
        "print(test_emb1.shape)\n",
        "print(train_emb1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clq8weCRkr_z",
        "outputId": "b72e350f-0f81-47da-9786-9136d475df4f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6250, 384)\n",
            "(18750, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to actually load in our model. We will first try a simple Linear model with Leaky Relu passes, which is implemented here."
      ],
      "metadata": {
        "id": "NFSHuzgRm3OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc3(F.leaky_relu(self.fc2(F.leaky_relu(self.fc1(x), 1)),1)),1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5bSx6njumSTE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to train our model, we use the training loop from P8."
      ],
      "metadata": {
        "id": "MSrMzpBpoeHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_classification_model(train_loader, model, loss_fn, num_epochs, lr=1e-2):\n",
        "  \"\"\"Train loop for a neural network model. Please use the SGD optimizer, optim.SGD.\n",
        "\n",
        "  Input:\n",
        "      train_loader:    Data loader for the train set.\n",
        "                        Enumerate through to train with each batch.\n",
        "      model:           nn.Model to be trained\n",
        "      num_epochs:      number of epochs to train the model for\n",
        "      lr:              learning rate for the optimizer\n",
        "      print_freq:      frequency to display the loss\n",
        "\n",
        "  Output:\n",
        "      model:   nn.Module trained model\n",
        "  \"\"\"\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Iterate through the dataloader for each epoch\n",
        "    for index, (batch, labels) in enumerate(train_loader):\n",
        "      # batch (torch.Tensor):    batch of r0 and r1\n",
        "      # labels (torch.Tensor):  batch labels corresponding to the inputs\n",
        "\n",
        "      # Implement the training loop using batch, labels, and cross entropy loss\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(batch).view(len(labels))\n",
        "\n",
        "      loss = loss_fn(preds, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "    print(\"Loss at Epoch \", epoch, \": \", loss.item())\n",
        "\n",
        "  return model  # return trained model"
      ],
      "metadata": {
        "id": "675jaQ4lnCdn"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's load our data and train our model! We normalize our data before training, as well as duplicating our data and re-ordering it with corresponding labels to add more training points."
      ],
      "metadata": {
        "id": "6RYWtOcvr5TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "n,d = train_emb1.shape\n",
        "print(n,d)\n",
        "\n",
        "print(train_emb1.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "train_emb1_tensor = torch.Tensor(np.concatenate((train_emb1, train_emb2), 0))\n",
        "train_emb2_tensor = torch.Tensor(np.concatenate((train_emb2, train_emb1), 0))\n",
        "train_emb_combined = torch.cat((train_emb1_tensor, train_emb2_tensor), 1)\n",
        "train_emb_combined_normalized = F.normalize(train_emb_combined)\n",
        "\n",
        "train_label_tensor = torch.Tensor(np.concatenate((train_labels, (train_labels == 0) * 1)))\n",
        "\n",
        "print(train_emb_combined.shape)\n",
        "print(train_label_tensor.shape)\n",
        "\n",
        "loader = data.DataLoader(data.TensorDataset(train_emb_combined_normalized, train_label_tensor), shuffle=True, batch_size=7000)\n",
        "\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "lstm_model = Model(d * 2, 70, 1)\n",
        "lstm_model = train_classification_model(loader, lstm_model, loss, 1000, lr=0.05)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buJuDZFNpNvX",
        "outputId": "1cc01d81-e7a7-41de-9aa6-aca475854fd1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18750 384\n",
            "(18750, 384)\n",
            "(18750,)\n",
            "torch.Size([37500, 768])\n",
            "torch.Size([37500])\n",
            "Loss at Epoch  0 :  0.6939914226531982\n",
            "Loss at Epoch  1 :  0.6948995590209961\n",
            "Loss at Epoch  2 :  0.6927148103713989\n",
            "Loss at Epoch  3 :  0.6943604350090027\n",
            "Loss at Epoch  4 :  0.6939060091972351\n",
            "Loss at Epoch  5 :  0.6936665177345276\n",
            "Loss at Epoch  6 :  0.6928666830062866\n",
            "Loss at Epoch  7 :  0.6933032870292664\n",
            "Loss at Epoch  8 :  0.6931398510932922\n",
            "Loss at Epoch  9 :  0.6928767561912537\n",
            "Loss at Epoch  10 :  0.6933670043945312\n",
            "Loss at Epoch  11 :  0.6929283738136292\n",
            "Loss at Epoch  12 :  0.6923529505729675\n",
            "Loss at Epoch  13 :  0.6926950216293335\n",
            "Loss at Epoch  14 :  0.6926024556159973\n",
            "Loss at Epoch  15 :  0.69264155626297\n",
            "Loss at Epoch  16 :  0.692297637462616\n",
            "Loss at Epoch  17 :  0.6921402215957642\n",
            "Loss at Epoch  18 :  0.6922172904014587\n",
            "Loss at Epoch  19 :  0.6921641826629639\n",
            "Loss at Epoch  20 :  0.692075252532959\n",
            "Loss at Epoch  21 :  0.6918979287147522\n",
            "Loss at Epoch  22 :  0.6920588612556458\n",
            "Loss at Epoch  23 :  0.69182950258255\n",
            "Loss at Epoch  24 :  0.6918525695800781\n",
            "Loss at Epoch  25 :  0.6918141841888428\n",
            "Loss at Epoch  26 :  0.6918732523918152\n",
            "Loss at Epoch  27 :  0.691668689250946\n",
            "Loss at Epoch  28 :  0.6915417909622192\n",
            "Loss at Epoch  29 :  0.6915445327758789\n",
            "Loss at Epoch  30 :  0.6913594007492065\n",
            "Loss at Epoch  31 :  0.6913915276527405\n",
            "Loss at Epoch  32 :  0.6912829279899597\n",
            "Loss at Epoch  33 :  0.6912477016448975\n",
            "Loss at Epoch  34 :  0.6909241080284119\n",
            "Loss at Epoch  35 :  0.6911216974258423\n",
            "Loss at Epoch  36 :  0.6909810304641724\n",
            "Loss at Epoch  37 :  0.6909031271934509\n",
            "Loss at Epoch  38 :  0.6907498240470886\n",
            "Loss at Epoch  39 :  0.6906733512878418\n",
            "Loss at Epoch  40 :  0.6906781792640686\n",
            "Loss at Epoch  41 :  0.6907705664634705\n",
            "Loss at Epoch  42 :  0.690327525138855\n",
            "Loss at Epoch  43 :  0.6904157996177673\n",
            "Loss at Epoch  44 :  0.6902824640274048\n",
            "Loss at Epoch  45 :  0.6903352737426758\n",
            "Loss at Epoch  46 :  0.6901476383209229\n",
            "Loss at Epoch  47 :  0.690069317817688\n",
            "Loss at Epoch  48 :  0.6900146007537842\n",
            "Loss at Epoch  49 :  0.6898588538169861\n",
            "Loss at Epoch  50 :  0.6896374225616455\n",
            "Loss at Epoch  51 :  0.6896276473999023\n",
            "Loss at Epoch  52 :  0.6895313262939453\n",
            "Loss at Epoch  53 :  0.6894026398658752\n",
            "Loss at Epoch  54 :  0.6893975138664246\n",
            "Loss at Epoch  55 :  0.6892762184143066\n",
            "Loss at Epoch  56 :  0.6891562342643738\n",
            "Loss at Epoch  57 :  0.6890621781349182\n",
            "Loss at Epoch  58 :  0.6888764500617981\n",
            "Loss at Epoch  59 :  0.6887524127960205\n",
            "Loss at Epoch  60 :  0.6887273192405701\n",
            "Loss at Epoch  61 :  0.6885952353477478\n",
            "Loss at Epoch  62 :  0.6885649561882019\n",
            "Loss at Epoch  63 :  0.6884363889694214\n",
            "Loss at Epoch  64 :  0.6879873871803284\n",
            "Loss at Epoch  65 :  0.6879412531852722\n",
            "Loss at Epoch  66 :  0.6880791187286377\n",
            "Loss at Epoch  67 :  0.6877790689468384\n",
            "Loss at Epoch  68 :  0.6875990033149719\n",
            "Loss at Epoch  69 :  0.687432050704956\n",
            "Loss at Epoch  70 :  0.6875912547111511\n",
            "Loss at Epoch  71 :  0.6874048113822937\n",
            "Loss at Epoch  72 :  0.6868751645088196\n",
            "Loss at Epoch  73 :  0.6868835687637329\n",
            "Loss at Epoch  74 :  0.6865165829658508\n",
            "Loss at Epoch  75 :  0.6864244341850281\n",
            "Loss at Epoch  76 :  0.6861468553543091\n",
            "Loss at Epoch  77 :  0.6863574385643005\n",
            "Loss at Epoch  78 :  0.6857865452766418\n",
            "Loss at Epoch  79 :  0.685858964920044\n",
            "Loss at Epoch  80 :  0.6859630346298218\n",
            "Loss at Epoch  81 :  0.6857245564460754\n",
            "Loss at Epoch  82 :  0.6855258345603943\n",
            "Loss at Epoch  83 :  0.6851619482040405\n",
            "Loss at Epoch  84 :  0.6845669150352478\n",
            "Loss at Epoch  85 :  0.6849969625473022\n",
            "Loss at Epoch  86 :  0.6843571662902832\n",
            "Loss at Epoch  87 :  0.6841375827789307\n",
            "Loss at Epoch  88 :  0.6838956475257874\n",
            "Loss at Epoch  89 :  0.6836592555046082\n",
            "Loss at Epoch  90 :  0.6834233999252319\n",
            "Loss at Epoch  91 :  0.682748019695282\n",
            "Loss at Epoch  92 :  0.6827768683433533\n",
            "Loss at Epoch  93 :  0.6824120283126831\n",
            "Loss at Epoch  94 :  0.6817296147346497\n",
            "Loss at Epoch  95 :  0.6817909479141235\n",
            "Loss at Epoch  96 :  0.681449294090271\n",
            "Loss at Epoch  97 :  0.6815380454063416\n",
            "Loss at Epoch  98 :  0.6807067394256592\n",
            "Loss at Epoch  99 :  0.6806408166885376\n",
            "Loss at Epoch  100 :  0.6807976365089417\n",
            "Loss at Epoch  101 :  0.6806145310401917\n",
            "Loss at Epoch  102 :  0.6800786852836609\n",
            "Loss at Epoch  103 :  0.6797045469284058\n",
            "Loss at Epoch  104 :  0.6793395280838013\n",
            "Loss at Epoch  105 :  0.6787087321281433\n",
            "Loss at Epoch  106 :  0.6781019568443298\n",
            "Loss at Epoch  107 :  0.6782965660095215\n",
            "Loss at Epoch  108 :  0.677061140537262\n",
            "Loss at Epoch  109 :  0.6763778924942017\n",
            "Loss at Epoch  110 :  0.6767659187316895\n",
            "Loss at Epoch  111 :  0.6762756109237671\n",
            "Loss at Epoch  112 :  0.6751862168312073\n",
            "Loss at Epoch  113 :  0.6745625734329224\n",
            "Loss at Epoch  114 :  0.6741721034049988\n",
            "Loss at Epoch  115 :  0.6740425229072571\n",
            "Loss at Epoch  116 :  0.673468291759491\n",
            "Loss at Epoch  117 :  0.6728600263595581\n",
            "Loss at Epoch  118 :  0.6721274256706238\n",
            "Loss at Epoch  119 :  0.6711735129356384\n",
            "Loss at Epoch  120 :  0.6705406308174133\n",
            "Loss at Epoch  121 :  0.6707987189292908\n",
            "Loss at Epoch  122 :  0.6703646779060364\n",
            "Loss at Epoch  123 :  0.6681718826293945\n",
            "Loss at Epoch  124 :  0.6675751805305481\n",
            "Loss at Epoch  125 :  0.6677294969558716\n",
            "Loss at Epoch  126 :  0.6658940315246582\n",
            "Loss at Epoch  127 :  0.6654379367828369\n",
            "Loss at Epoch  128 :  0.6656478643417358\n",
            "Loss at Epoch  129 :  0.6643255949020386\n",
            "Loss at Epoch  130 :  0.6635197401046753\n",
            "Loss at Epoch  131 :  0.6628867387771606\n",
            "Loss at Epoch  132 :  0.6607288122177124\n",
            "Loss at Epoch  133 :  0.6610621213912964\n",
            "Loss at Epoch  134 :  0.6596518158912659\n",
            "Loss at Epoch  135 :  0.6587340831756592\n",
            "Loss at Epoch  136 :  0.6573467254638672\n",
            "Loss at Epoch  137 :  0.6561315655708313\n",
            "Loss at Epoch  138 :  0.6567335724830627\n",
            "Loss at Epoch  139 :  0.6546620726585388\n",
            "Loss at Epoch  140 :  0.6532047986984253\n",
            "Loss at Epoch  141 :  0.6514925956726074\n",
            "Loss at Epoch  142 :  0.6490781903266907\n",
            "Loss at Epoch  143 :  0.6498091816902161\n",
            "Loss at Epoch  144 :  0.6482149362564087\n",
            "Loss at Epoch  145 :  0.6444644927978516\n",
            "Loss at Epoch  146 :  0.6441075205802917\n",
            "Loss at Epoch  147 :  0.641287624835968\n",
            "Loss at Epoch  148 :  0.6413376331329346\n",
            "Loss at Epoch  149 :  0.6380634307861328\n",
            "Loss at Epoch  150 :  0.6362050771713257\n",
            "Loss at Epoch  151 :  0.6345539093017578\n",
            "Loss at Epoch  152 :  0.6354396343231201\n",
            "Loss at Epoch  153 :  0.6307624578475952\n",
            "Loss at Epoch  154 :  0.6299617886543274\n",
            "Loss at Epoch  155 :  0.6282042264938354\n",
            "Loss at Epoch  156 :  0.6256836652755737\n",
            "Loss at Epoch  157 :  0.623477041721344\n",
            "Loss at Epoch  158 :  0.6188319325447083\n",
            "Loss at Epoch  159 :  0.6175611019134521\n",
            "Loss at Epoch  160 :  0.6133192777633667\n",
            "Loss at Epoch  161 :  0.6109667420387268\n",
            "Loss at Epoch  162 :  0.6095784306526184\n",
            "Loss at Epoch  163 :  0.6084591150283813\n",
            "Loss at Epoch  164 :  0.6024515628814697\n",
            "Loss at Epoch  165 :  0.6026586890220642\n",
            "Loss at Epoch  166 :  0.5950102210044861\n",
            "Loss at Epoch  167 :  0.5922722220420837\n",
            "Loss at Epoch  168 :  0.5883751511573792\n",
            "Loss at Epoch  169 :  0.588851273059845\n",
            "Loss at Epoch  170 :  0.5830284953117371\n",
            "Loss at Epoch  171 :  0.5807755589485168\n",
            "Loss at Epoch  172 :  0.5781197547912598\n",
            "Loss at Epoch  173 :  0.5741576552391052\n",
            "Loss at Epoch  174 :  0.5693317651748657\n",
            "Loss at Epoch  175 :  0.5654014348983765\n",
            "Loss at Epoch  176 :  0.5657019019126892\n",
            "Loss at Epoch  177 :  0.5605695247650146\n",
            "Loss at Epoch  178 :  0.5514034032821655\n",
            "Loss at Epoch  179 :  0.5503054857254028\n",
            "Loss at Epoch  180 :  0.5450230240821838\n",
            "Loss at Epoch  181 :  0.5426845550537109\n",
            "Loss at Epoch  182 :  0.5338841080665588\n",
            "Loss at Epoch  183 :  0.5300352573394775\n",
            "Loss at Epoch  184 :  0.5347386598587036\n",
            "Loss at Epoch  185 :  0.5198861360549927\n",
            "Loss at Epoch  186 :  0.515550434589386\n",
            "Loss at Epoch  187 :  0.5230267643928528\n",
            "Loss at Epoch  188 :  0.5110378861427307\n",
            "Loss at Epoch  189 :  0.5019826292991638\n",
            "Loss at Epoch  190 :  0.4997849464416504\n",
            "Loss at Epoch  191 :  0.496060311794281\n",
            "Loss at Epoch  192 :  0.4916313886642456\n",
            "Loss at Epoch  193 :  0.48210981488227844\n",
            "Loss at Epoch  194 :  0.4712766706943512\n",
            "Loss at Epoch  195 :  0.48057466745376587\n",
            "Loss at Epoch  196 :  0.47017210721969604\n",
            "Loss at Epoch  197 :  0.46343663334846497\n",
            "Loss at Epoch  198 :  0.46156251430511475\n",
            "Loss at Epoch  199 :  0.44532763957977295\n",
            "Loss at Epoch  200 :  0.455293744802475\n",
            "Loss at Epoch  201 :  0.44780293107032776\n",
            "Loss at Epoch  202 :  0.44375449419021606\n",
            "Loss at Epoch  203 :  0.4313114881515503\n",
            "Loss at Epoch  204 :  0.4253475069999695\n",
            "Loss at Epoch  205 :  0.43587154150009155\n",
            "Loss at Epoch  206 :  0.42545175552368164\n",
            "Loss at Epoch  207 :  0.4279329478740692\n",
            "Loss at Epoch  208 :  0.4128868579864502\n",
            "Loss at Epoch  209 :  0.4236252009868622\n",
            "Loss at Epoch  210 :  0.4114260673522949\n",
            "Loss at Epoch  211 :  0.41359618306159973\n",
            "Loss at Epoch  212 :  0.4174838364124298\n",
            "Loss at Epoch  213 :  0.4047190546989441\n",
            "Loss at Epoch  214 :  0.399765282869339\n",
            "Loss at Epoch  215 :  0.39097100496292114\n",
            "Loss at Epoch  216 :  0.3821580410003662\n",
            "Loss at Epoch  217 :  0.38322317600250244\n",
            "Loss at Epoch  218 :  0.3911046087741852\n",
            "Loss at Epoch  219 :  0.3878081440925598\n",
            "Loss at Epoch  220 :  0.38367709517478943\n",
            "Loss at Epoch  221 :  0.39171090722084045\n",
            "Loss at Epoch  222 :  0.3703472912311554\n",
            "Loss at Epoch  223 :  0.367809921503067\n",
            "Loss at Epoch  224 :  0.3708970844745636\n",
            "Loss at Epoch  225 :  0.3617091774940491\n",
            "Loss at Epoch  226 :  0.3763827085494995\n",
            "Loss at Epoch  227 :  0.36728858947753906\n",
            "Loss at Epoch  228 :  0.3650962710380554\n",
            "Loss at Epoch  229 :  0.3586183488368988\n",
            "Loss at Epoch  230 :  0.3574972152709961\n",
            "Loss at Epoch  231 :  0.3627357482910156\n",
            "Loss at Epoch  232 :  0.3425205945968628\n",
            "Loss at Epoch  233 :  0.3582135736942291\n",
            "Loss at Epoch  234 :  0.3420267701148987\n",
            "Loss at Epoch  235 :  0.35858070850372314\n",
            "Loss at Epoch  236 :  0.3584381341934204\n",
            "Loss at Epoch  237 :  0.3485977053642273\n",
            "Loss at Epoch  238 :  0.35649508237838745\n",
            "Loss at Epoch  239 :  0.3364272713661194\n",
            "Loss at Epoch  240 :  0.3415113389492035\n",
            "Loss at Epoch  241 :  0.3484037518501282\n",
            "Loss at Epoch  242 :  0.3377152979373932\n",
            "Loss at Epoch  243 :  0.3506653904914856\n",
            "Loss at Epoch  244 :  0.3428141474723816\n",
            "Loss at Epoch  245 :  0.34076324105262756\n",
            "Loss at Epoch  246 :  0.3352230191230774\n",
            "Loss at Epoch  247 :  0.3287757337093353\n",
            "Loss at Epoch  248 :  0.32118186354637146\n",
            "Loss at Epoch  249 :  0.3271901607513428\n",
            "Loss at Epoch  250 :  0.3339819312095642\n",
            "Loss at Epoch  251 :  0.3333771824836731\n",
            "Loss at Epoch  252 :  0.33181098103523254\n",
            "Loss at Epoch  253 :  0.3077722191810608\n",
            "Loss at Epoch  254 :  0.32601988315582275\n",
            "Loss at Epoch  255 :  0.31055882573127747\n",
            "Loss at Epoch  256 :  0.3213581442832947\n",
            "Loss at Epoch  257 :  0.33998551964759827\n",
            "Loss at Epoch  258 :  0.31597986817359924\n",
            "Loss at Epoch  259 :  0.33120039105415344\n",
            "Loss at Epoch  260 :  0.31551405787467957\n",
            "Loss at Epoch  261 :  0.3241097033023834\n",
            "Loss at Epoch  262 :  0.32262304425239563\n",
            "Loss at Epoch  263 :  0.31837764382362366\n",
            "Loss at Epoch  264 :  0.3032132387161255\n",
            "Loss at Epoch  265 :  0.31147003173828125\n",
            "Loss at Epoch  266 :  0.3139783442020416\n",
            "Loss at Epoch  267 :  0.31950822472572327\n",
            "Loss at Epoch  268 :  0.3116788864135742\n",
            "Loss at Epoch  269 :  0.30691757798194885\n",
            "Loss at Epoch  270 :  0.3126859664916992\n",
            "Loss at Epoch  271 :  0.3172590434551239\n",
            "Loss at Epoch  272 :  0.30853307247161865\n",
            "Loss at Epoch  273 :  0.2996964752674103\n",
            "Loss at Epoch  274 :  0.3051174581050873\n",
            "Loss at Epoch  275 :  0.29629018902778625\n",
            "Loss at Epoch  276 :  0.30197665095329285\n",
            "Loss at Epoch  277 :  0.3176003098487854\n",
            "Loss at Epoch  278 :  0.2994285821914673\n",
            "Loss at Epoch  279 :  0.2991534173488617\n",
            "Loss at Epoch  280 :  0.3137299716472626\n",
            "Loss at Epoch  281 :  0.3080718517303467\n",
            "Loss at Epoch  282 :  0.29832327365875244\n",
            "Loss at Epoch  283 :  0.3025650382041931\n",
            "Loss at Epoch  284 :  0.309321790933609\n",
            "Loss at Epoch  285 :  0.30443039536476135\n",
            "Loss at Epoch  286 :  0.301319420337677\n",
            "Loss at Epoch  287 :  0.31353914737701416\n",
            "Loss at Epoch  288 :  0.30339428782463074\n",
            "Loss at Epoch  289 :  0.28637656569480896\n",
            "Loss at Epoch  290 :  0.29893627762794495\n",
            "Loss at Epoch  291 :  0.2935011386871338\n",
            "Loss at Epoch  292 :  0.30264946818351746\n",
            "Loss at Epoch  293 :  0.295941561460495\n",
            "Loss at Epoch  294 :  0.28943076729774475\n",
            "Loss at Epoch  295 :  0.310491681098938\n",
            "Loss at Epoch  296 :  0.27781087160110474\n",
            "Loss at Epoch  297 :  0.2768629193305969\n",
            "Loss at Epoch  298 :  0.3013923466205597\n",
            "Loss at Epoch  299 :  0.311064213514328\n",
            "Loss at Epoch  300 :  0.3024708926677704\n",
            "Loss at Epoch  301 :  0.2968614399433136\n",
            "Loss at Epoch  302 :  0.29097703099250793\n",
            "Loss at Epoch  303 :  0.2958414554595947\n",
            "Loss at Epoch  304 :  0.30833402276039124\n",
            "Loss at Epoch  305 :  0.2906745672225952\n",
            "Loss at Epoch  306 :  0.2875799238681793\n",
            "Loss at Epoch  307 :  0.2859913110733032\n",
            "Loss at Epoch  308 :  0.2824386954307556\n",
            "Loss at Epoch  309 :  0.2863074541091919\n",
            "Loss at Epoch  310 :  0.27676334977149963\n",
            "Loss at Epoch  311 :  0.2836700975894928\n",
            "Loss at Epoch  312 :  0.27747610211372375\n",
            "Loss at Epoch  313 :  0.30160805583000183\n",
            "Loss at Epoch  314 :  0.3042331635951996\n",
            "Loss at Epoch  315 :  0.29003238677978516\n",
            "Loss at Epoch  316 :  0.2917267680168152\n",
            "Loss at Epoch  317 :  0.29432398080825806\n",
            "Loss at Epoch  318 :  0.2879638075828552\n",
            "Loss at Epoch  319 :  0.28798550367355347\n",
            "Loss at Epoch  320 :  0.28392642736434937\n",
            "Loss at Epoch  321 :  0.2745268642902374\n",
            "Loss at Epoch  322 :  0.28593677282333374\n",
            "Loss at Epoch  323 :  0.2859412133693695\n",
            "Loss at Epoch  324 :  0.2769585847854614\n",
            "Loss at Epoch  325 :  0.287434846162796\n",
            "Loss at Epoch  326 :  0.2839490473270416\n",
            "Loss at Epoch  327 :  0.2679150402545929\n",
            "Loss at Epoch  328 :  0.2824268341064453\n",
            "Loss at Epoch  329 :  0.2681354880332947\n",
            "Loss at Epoch  330 :  0.2758844196796417\n",
            "Loss at Epoch  331 :  0.2713780403137207\n",
            "Loss at Epoch  332 :  0.2813372313976288\n",
            "Loss at Epoch  333 :  0.26170793175697327\n",
            "Loss at Epoch  334 :  0.27059921622276306\n",
            "Loss at Epoch  335 :  0.26027795672416687\n",
            "Loss at Epoch  336 :  0.2853139042854309\n",
            "Loss at Epoch  337 :  0.27706459164619446\n",
            "Loss at Epoch  338 :  0.279887855052948\n",
            "Loss at Epoch  339 :  0.2689497470855713\n",
            "Loss at Epoch  340 :  0.27690544724464417\n",
            "Loss at Epoch  341 :  0.28879019618034363\n",
            "Loss at Epoch  342 :  0.2902836799621582\n",
            "Loss at Epoch  343 :  0.3005032539367676\n",
            "Loss at Epoch  344 :  0.2800142467021942\n",
            "Loss at Epoch  345 :  0.266465961933136\n",
            "Loss at Epoch  346 :  0.26487088203430176\n",
            "Loss at Epoch  347 :  0.2609928846359253\n",
            "Loss at Epoch  348 :  0.2713034152984619\n",
            "Loss at Epoch  349 :  0.2687557339668274\n",
            "Loss at Epoch  350 :  0.2681945264339447\n",
            "Loss at Epoch  351 :  0.28506767749786377\n",
            "Loss at Epoch  352 :  0.27221572399139404\n",
            "Loss at Epoch  353 :  0.27768877148628235\n",
            "Loss at Epoch  354 :  0.27930229902267456\n",
            "Loss at Epoch  355 :  0.26782846450805664\n",
            "Loss at Epoch  356 :  0.2770634889602661\n",
            "Loss at Epoch  357 :  0.28047993779182434\n",
            "Loss at Epoch  358 :  0.2648467421531677\n",
            "Loss at Epoch  359 :  0.2789670526981354\n",
            "Loss at Epoch  360 :  0.2685817778110504\n",
            "Loss at Epoch  361 :  0.2533515989780426\n",
            "Loss at Epoch  362 :  0.2816604971885681\n",
            "Loss at Epoch  363 :  0.28992795944213867\n",
            "Loss at Epoch  364 :  0.2771774232387543\n",
            "Loss at Epoch  365 :  0.27088767290115356\n",
            "Loss at Epoch  366 :  0.24747733771800995\n",
            "Loss at Epoch  367 :  0.2626560926437378\n",
            "Loss at Epoch  368 :  0.27620744705200195\n",
            "Loss at Epoch  369 :  0.2801976799964905\n",
            "Loss at Epoch  370 :  0.25479617714881897\n",
            "Loss at Epoch  371 :  0.269823282957077\n",
            "Loss at Epoch  372 :  0.2474471479654312\n",
            "Loss at Epoch  373 :  0.2659318447113037\n",
            "Loss at Epoch  374 :  0.2702804505825043\n",
            "Loss at Epoch  375 :  0.2594273090362549\n",
            "Loss at Epoch  376 :  0.27500301599502563\n",
            "Loss at Epoch  377 :  0.2724165916442871\n",
            "Loss at Epoch  378 :  0.2626957893371582\n",
            "Loss at Epoch  379 :  0.2594105899333954\n",
            "Loss at Epoch  380 :  0.275508850812912\n",
            "Loss at Epoch  381 :  0.26519715785980225\n",
            "Loss at Epoch  382 :  0.2561470568180084\n",
            "Loss at Epoch  383 :  0.2737729251384735\n",
            "Loss at Epoch  384 :  0.2762417495250702\n",
            "Loss at Epoch  385 :  0.2562624514102936\n",
            "Loss at Epoch  386 :  0.2611568868160248\n",
            "Loss at Epoch  387 :  0.274341881275177\n",
            "Loss at Epoch  388 :  0.2687167525291443\n",
            "Loss at Epoch  389 :  0.24849072098731995\n",
            "Loss at Epoch  390 :  0.26285645365715027\n",
            "Loss at Epoch  391 :  0.2685548961162567\n",
            "Loss at Epoch  392 :  0.25212734937667847\n",
            "Loss at Epoch  393 :  0.2680790424346924\n",
            "Loss at Epoch  394 :  0.2768748998641968\n",
            "Loss at Epoch  395 :  0.2513834834098816\n",
            "Loss at Epoch  396 :  0.2713801860809326\n",
            "Loss at Epoch  397 :  0.24905817210674286\n",
            "Loss at Epoch  398 :  0.25031840801239014\n",
            "Loss at Epoch  399 :  0.24929550290107727\n",
            "Loss at Epoch  400 :  0.27429211139678955\n",
            "Loss at Epoch  401 :  0.2692870795726776\n",
            "Loss at Epoch  402 :  0.24762773513793945\n",
            "Loss at Epoch  403 :  0.2560223340988159\n",
            "Loss at Epoch  404 :  0.2666975259780884\n",
            "Loss at Epoch  405 :  0.2572439908981323\n",
            "Loss at Epoch  406 :  0.2556973993778229\n",
            "Loss at Epoch  407 :  0.2721029818058014\n",
            "Loss at Epoch  408 :  0.2690696120262146\n",
            "Loss at Epoch  409 :  0.2570514678955078\n",
            "Loss at Epoch  410 :  0.26847583055496216\n",
            "Loss at Epoch  411 :  0.2609510123729706\n",
            "Loss at Epoch  412 :  0.2725898325443268\n",
            "Loss at Epoch  413 :  0.25455519556999207\n",
            "Loss at Epoch  414 :  0.2702937126159668\n",
            "Loss at Epoch  415 :  0.25579652190208435\n",
            "Loss at Epoch  416 :  0.26228398084640503\n",
            "Loss at Epoch  417 :  0.264209508895874\n",
            "Loss at Epoch  418 :  0.26526686549186707\n",
            "Loss at Epoch  419 :  0.25104618072509766\n",
            "Loss at Epoch  420 :  0.26463884115219116\n",
            "Loss at Epoch  421 :  0.2638777196407318\n",
            "Loss at Epoch  422 :  0.25925227999687195\n",
            "Loss at Epoch  423 :  0.23965029418468475\n",
            "Loss at Epoch  424 :  0.25878968834877014\n",
            "Loss at Epoch  425 :  0.2537802755832672\n",
            "Loss at Epoch  426 :  0.2563180923461914\n",
            "Loss at Epoch  427 :  0.2706510126590729\n",
            "Loss at Epoch  428 :  0.24045944213867188\n",
            "Loss at Epoch  429 :  0.2469974160194397\n",
            "Loss at Epoch  430 :  0.2537972033023834\n",
            "Loss at Epoch  431 :  0.24729862809181213\n",
            "Loss at Epoch  432 :  0.27263233065605164\n",
            "Loss at Epoch  433 :  0.24411223828792572\n",
            "Loss at Epoch  434 :  0.2652653157711029\n",
            "Loss at Epoch  435 :  0.2592068910598755\n",
            "Loss at Epoch  436 :  0.2541016638278961\n",
            "Loss at Epoch  437 :  0.25103917717933655\n",
            "Loss at Epoch  438 :  0.2673940062522888\n",
            "Loss at Epoch  439 :  0.2663330137729645\n",
            "Loss at Epoch  440 :  0.25396987795829773\n",
            "Loss at Epoch  441 :  0.2598048448562622\n",
            "Loss at Epoch  442 :  0.26168614625930786\n",
            "Loss at Epoch  443 :  0.25681814551353455\n",
            "Loss at Epoch  444 :  0.2585653066635132\n",
            "Loss at Epoch  445 :  0.2650023400783539\n",
            "Loss at Epoch  446 :  0.25824788212776184\n",
            "Loss at Epoch  447 :  0.26184824109077454\n",
            "Loss at Epoch  448 :  0.2481241226196289\n",
            "Loss at Epoch  449 :  0.25632667541503906\n",
            "Loss at Epoch  450 :  0.24772074818611145\n",
            "Loss at Epoch  451 :  0.22866764664649963\n",
            "Loss at Epoch  452 :  0.2517494559288025\n",
            "Loss at Epoch  453 :  0.24702167510986328\n",
            "Loss at Epoch  454 :  0.2720421552658081\n",
            "Loss at Epoch  455 :  0.2542848289012909\n",
            "Loss at Epoch  456 :  0.24604962766170502\n",
            "Loss at Epoch  457 :  0.24694985151290894\n",
            "Loss at Epoch  458 :  0.2520098388195038\n",
            "Loss at Epoch  459 :  0.24901698529720306\n",
            "Loss at Epoch  460 :  0.24278110265731812\n",
            "Loss at Epoch  461 :  0.26745110750198364\n",
            "Loss at Epoch  462 :  0.2528153359889984\n",
            "Loss at Epoch  463 :  0.24190430343151093\n",
            "Loss at Epoch  464 :  0.24788649380207062\n",
            "Loss at Epoch  465 :  0.27494221925735474\n",
            "Loss at Epoch  466 :  0.24770858883857727\n",
            "Loss at Epoch  467 :  0.2461378425359726\n",
            "Loss at Epoch  468 :  0.25614210963249207\n",
            "Loss at Epoch  469 :  0.25579747557640076\n",
            "Loss at Epoch  470 :  0.2628374397754669\n",
            "Loss at Epoch  471 :  0.24799853563308716\n",
            "Loss at Epoch  472 :  0.238891139626503\n",
            "Loss at Epoch  473 :  0.25318098068237305\n",
            "Loss at Epoch  474 :  0.25519031286239624\n",
            "Loss at Epoch  475 :  0.2597752511501312\n",
            "Loss at Epoch  476 :  0.2517510950565338\n",
            "Loss at Epoch  477 :  0.24612969160079956\n",
            "Loss at Epoch  478 :  0.2687647044658661\n",
            "Loss at Epoch  479 :  0.24571697413921356\n",
            "Loss at Epoch  480 :  0.24700653553009033\n",
            "Loss at Epoch  481 :  0.2323582023382187\n",
            "Loss at Epoch  482 :  0.2505887448787689\n",
            "Loss at Epoch  483 :  0.24218003451824188\n",
            "Loss at Epoch  484 :  0.25352269411087036\n",
            "Loss at Epoch  485 :  0.2366609424352646\n",
            "Loss at Epoch  486 :  0.255323201417923\n",
            "Loss at Epoch  487 :  0.25000572204589844\n",
            "Loss at Epoch  488 :  0.25312986969947815\n",
            "Loss at Epoch  489 :  0.2376030534505844\n",
            "Loss at Epoch  490 :  0.24146440625190735\n",
            "Loss at Epoch  491 :  0.24472589790821075\n",
            "Loss at Epoch  492 :  0.2564718723297119\n",
            "Loss at Epoch  493 :  0.254577100276947\n",
            "Loss at Epoch  494 :  0.23413154482841492\n",
            "Loss at Epoch  495 :  0.25848791003227234\n",
            "Loss at Epoch  496 :  0.24211212992668152\n",
            "Loss at Epoch  497 :  0.25700071454048157\n",
            "Loss at Epoch  498 :  0.2471473217010498\n",
            "Loss at Epoch  499 :  0.2674205005168915\n",
            "Loss at Epoch  500 :  0.2334510236978531\n",
            "Loss at Epoch  501 :  0.24600820243358612\n",
            "Loss at Epoch  502 :  0.24091173708438873\n",
            "Loss at Epoch  503 :  0.24274370074272156\n",
            "Loss at Epoch  504 :  0.25210821628570557\n",
            "Loss at Epoch  505 :  0.24217261373996735\n",
            "Loss at Epoch  506 :  0.24659231305122375\n",
            "Loss at Epoch  507 :  0.2584804594516754\n",
            "Loss at Epoch  508 :  0.25217288732528687\n",
            "Loss at Epoch  509 :  0.26266562938690186\n",
            "Loss at Epoch  510 :  0.2534853518009186\n",
            "Loss at Epoch  511 :  0.2551780641078949\n",
            "Loss at Epoch  512 :  0.2655119299888611\n",
            "Loss at Epoch  513 :  0.24959999322891235\n",
            "Loss at Epoch  514 :  0.25945746898651123\n",
            "Loss at Epoch  515 :  0.2346055656671524\n",
            "Loss at Epoch  516 :  0.24912326037883759\n",
            "Loss at Epoch  517 :  0.25286391377449036\n",
            "Loss at Epoch  518 :  0.261432945728302\n",
            "Loss at Epoch  519 :  0.23967209458351135\n",
            "Loss at Epoch  520 :  0.2571970522403717\n",
            "Loss at Epoch  521 :  0.2482982873916626\n",
            "Loss at Epoch  522 :  0.2604231536388397\n",
            "Loss at Epoch  523 :  0.2565261721611023\n",
            "Loss at Epoch  524 :  0.24416705965995789\n",
            "Loss at Epoch  525 :  0.24069848656654358\n",
            "Loss at Epoch  526 :  0.24399560689926147\n",
            "Loss at Epoch  527 :  0.23763048648834229\n",
            "Loss at Epoch  528 :  0.25955328345298767\n",
            "Loss at Epoch  529 :  0.24908970296382904\n",
            "Loss at Epoch  530 :  0.23996201157569885\n",
            "Loss at Epoch  531 :  0.25537413358688354\n",
            "Loss at Epoch  532 :  0.24742966890335083\n",
            "Loss at Epoch  533 :  0.2639657258987427\n",
            "Loss at Epoch  534 :  0.24530407786369324\n",
            "Loss at Epoch  535 :  0.2176479995250702\n",
            "Loss at Epoch  536 :  0.2412635236978531\n",
            "Loss at Epoch  537 :  0.2467162162065506\n",
            "Loss at Epoch  538 :  0.24810779094696045\n",
            "Loss at Epoch  539 :  0.24630790948867798\n",
            "Loss at Epoch  540 :  0.26293689012527466\n",
            "Loss at Epoch  541 :  0.24691447615623474\n",
            "Loss at Epoch  542 :  0.24837392568588257\n",
            "Loss at Epoch  543 :  0.2578935921192169\n",
            "Loss at Epoch  544 :  0.24545298516750336\n",
            "Loss at Epoch  545 :  0.2619989216327667\n",
            "Loss at Epoch  546 :  0.2463841736316681\n",
            "Loss at Epoch  547 :  0.2364228069782257\n",
            "Loss at Epoch  548 :  0.25694161653518677\n",
            "Loss at Epoch  549 :  0.26207128167152405\n",
            "Loss at Epoch  550 :  0.24115212261676788\n",
            "Loss at Epoch  551 :  0.25248369574546814\n",
            "Loss at Epoch  552 :  0.2396315634250641\n",
            "Loss at Epoch  553 :  0.23838120698928833\n",
            "Loss at Epoch  554 :  0.2410423308610916\n",
            "Loss at Epoch  555 :  0.23801034688949585\n",
            "Loss at Epoch  556 :  0.24939091503620148\n",
            "Loss at Epoch  557 :  0.2498713880777359\n",
            "Loss at Epoch  558 :  0.23417514562606812\n",
            "Loss at Epoch  559 :  0.2412841022014618\n",
            "Loss at Epoch  560 :  0.25636258721351624\n",
            "Loss at Epoch  561 :  0.24597524106502533\n",
            "Loss at Epoch  562 :  0.23477627336978912\n",
            "Loss at Epoch  563 :  0.25688377022743225\n",
            "Loss at Epoch  564 :  0.2401488572359085\n",
            "Loss at Epoch  565 :  0.23590879142284393\n",
            "Loss at Epoch  566 :  0.23830220103263855\n",
            "Loss at Epoch  567 :  0.23234499990940094\n",
            "Loss at Epoch  568 :  0.23537546396255493\n",
            "Loss at Epoch  569 :  0.2426355928182602\n",
            "Loss at Epoch  570 :  0.24377253651618958\n",
            "Loss at Epoch  571 :  0.2618755102157593\n",
            "Loss at Epoch  572 :  0.23767147958278656\n",
            "Loss at Epoch  573 :  0.24163416028022766\n",
            "Loss at Epoch  574 :  0.23106835782527924\n",
            "Loss at Epoch  575 :  0.24524043500423431\n",
            "Loss at Epoch  576 :  0.2386363297700882\n",
            "Loss at Epoch  577 :  0.24432484805583954\n",
            "Loss at Epoch  578 :  0.24083098769187927\n",
            "Loss at Epoch  579 :  0.24277529120445251\n",
            "Loss at Epoch  580 :  0.2567303776741028\n",
            "Loss at Epoch  581 :  0.2557832896709442\n",
            "Loss at Epoch  582 :  0.253200888633728\n",
            "Loss at Epoch  583 :  0.23781462013721466\n",
            "Loss at Epoch  584 :  0.2399958223104477\n",
            "Loss at Epoch  585 :  0.24234914779663086\n",
            "Loss at Epoch  586 :  0.227660670876503\n",
            "Loss at Epoch  587 :  0.2373189926147461\n",
            "Loss at Epoch  588 :  0.24655590951442719\n",
            "Loss at Epoch  589 :  0.25600600242614746\n",
            "Loss at Epoch  590 :  0.2630693316459656\n",
            "Loss at Epoch  591 :  0.24013391137123108\n",
            "Loss at Epoch  592 :  0.22553576529026031\n",
            "Loss at Epoch  593 :  0.24150264263153076\n",
            "Loss at Epoch  594 :  0.24570083618164062\n",
            "Loss at Epoch  595 :  0.24846334755420685\n",
            "Loss at Epoch  596 :  0.2452923059463501\n",
            "Loss at Epoch  597 :  0.2534816563129425\n",
            "Loss at Epoch  598 :  0.23278090357780457\n",
            "Loss at Epoch  599 :  0.23036885261535645\n",
            "Loss at Epoch  600 :  0.2621960937976837\n",
            "Loss at Epoch  601 :  0.24465326964855194\n",
            "Loss at Epoch  602 :  0.2384023368358612\n",
            "Loss at Epoch  603 :  0.2520125210285187\n",
            "Loss at Epoch  604 :  0.2639719545841217\n",
            "Loss at Epoch  605 :  0.23803485929965973\n",
            "Loss at Epoch  606 :  0.23847231268882751\n",
            "Loss at Epoch  607 :  0.24203039705753326\n",
            "Loss at Epoch  608 :  0.22342859208583832\n",
            "Loss at Epoch  609 :  0.23759719729423523\n",
            "Loss at Epoch  610 :  0.24082094430923462\n",
            "Loss at Epoch  611 :  0.26148369908332825\n",
            "Loss at Epoch  612 :  0.2550814747810364\n",
            "Loss at Epoch  613 :  0.24473614990711212\n",
            "Loss at Epoch  614 :  0.24191631376743317\n",
            "Loss at Epoch  615 :  0.25008562207221985\n",
            "Loss at Epoch  616 :  0.24947932362556458\n",
            "Loss at Epoch  617 :  0.23824629187583923\n",
            "Loss at Epoch  618 :  0.24938493967056274\n",
            "Loss at Epoch  619 :  0.2389514446258545\n",
            "Loss at Epoch  620 :  0.2555132508277893\n",
            "Loss at Epoch  621 :  0.23494453728199005\n",
            "Loss at Epoch  622 :  0.24173462390899658\n",
            "Loss at Epoch  623 :  0.2513357996940613\n",
            "Loss at Epoch  624 :  0.23296381533145905\n",
            "Loss at Epoch  625 :  0.24656890332698822\n",
            "Loss at Epoch  626 :  0.2532700300216675\n",
            "Loss at Epoch  627 :  0.2544814348220825\n",
            "Loss at Epoch  628 :  0.23559211194515228\n",
            "Loss at Epoch  629 :  0.24184423685073853\n",
            "Loss at Epoch  630 :  0.24955761432647705\n",
            "Loss at Epoch  631 :  0.2637508809566498\n",
            "Loss at Epoch  632 :  0.24154038727283478\n",
            "Loss at Epoch  633 :  0.23383119702339172\n",
            "Loss at Epoch  634 :  0.2457335889339447\n",
            "Loss at Epoch  635 :  0.23137128353118896\n",
            "Loss at Epoch  636 :  0.2338668256998062\n",
            "Loss at Epoch  637 :  0.23752787709236145\n",
            "Loss at Epoch  638 :  0.24523091316223145\n",
            "Loss at Epoch  639 :  0.24655812978744507\n",
            "Loss at Epoch  640 :  0.2560465931892395\n",
            "Loss at Epoch  641 :  0.24041159451007843\n",
            "Loss at Epoch  642 :  0.24361222982406616\n",
            "Loss at Epoch  643 :  0.2648177146911621\n",
            "Loss at Epoch  644 :  0.25321778655052185\n",
            "Loss at Epoch  645 :  0.2477373480796814\n",
            "Loss at Epoch  646 :  0.24017438292503357\n",
            "Loss at Epoch  647 :  0.2266821563243866\n",
            "Loss at Epoch  648 :  0.24891288578510284\n",
            "Loss at Epoch  649 :  0.24825698137283325\n",
            "Loss at Epoch  650 :  0.25153419375419617\n",
            "Loss at Epoch  651 :  0.23851992189884186\n",
            "Loss at Epoch  652 :  0.21769630908966064\n",
            "Loss at Epoch  653 :  0.24136120080947876\n",
            "Loss at Epoch  654 :  0.24536676704883575\n",
            "Loss at Epoch  655 :  0.22995957732200623\n",
            "Loss at Epoch  656 :  0.2494606375694275\n",
            "Loss at Epoch  657 :  0.23914621770381927\n",
            "Loss at Epoch  658 :  0.24498827755451202\n",
            "Loss at Epoch  659 :  0.22487914562225342\n",
            "Loss at Epoch  660 :  0.24671728909015656\n",
            "Loss at Epoch  661 :  0.23532222211360931\n",
            "Loss at Epoch  662 :  0.24020543694496155\n",
            "Loss at Epoch  663 :  0.23781442642211914\n",
            "Loss at Epoch  664 :  0.23904328048229218\n",
            "Loss at Epoch  665 :  0.237533837556839\n",
            "Loss at Epoch  666 :  0.23949936032295227\n",
            "Loss at Epoch  667 :  0.24728435277938843\n",
            "Loss at Epoch  668 :  0.2278747856616974\n",
            "Loss at Epoch  669 :  0.24344892799854279\n",
            "Loss at Epoch  670 :  0.24926620721817017\n",
            "Loss at Epoch  671 :  0.22668932378292084\n",
            "Loss at Epoch  672 :  0.2390236109495163\n",
            "Loss at Epoch  673 :  0.23047292232513428\n",
            "Loss at Epoch  674 :  0.22078484296798706\n",
            "Loss at Epoch  675 :  0.24181555211544037\n",
            "Loss at Epoch  676 :  0.24439135193824768\n",
            "Loss at Epoch  677 :  0.2410721629858017\n",
            "Loss at Epoch  678 :  0.2536751925945282\n",
            "Loss at Epoch  679 :  0.22505711019039154\n",
            "Loss at Epoch  680 :  0.251431405544281\n",
            "Loss at Epoch  681 :  0.24403691291809082\n",
            "Loss at Epoch  682 :  0.23623771965503693\n",
            "Loss at Epoch  683 :  0.23971760272979736\n",
            "Loss at Epoch  684 :  0.2663867175579071\n",
            "Loss at Epoch  685 :  0.24121098220348358\n",
            "Loss at Epoch  686 :  0.2308637648820877\n",
            "Loss at Epoch  687 :  0.2429801970720291\n",
            "Loss at Epoch  688 :  0.24224211275577545\n",
            "Loss at Epoch  689 :  0.24921220541000366\n",
            "Loss at Epoch  690 :  0.24367958307266235\n",
            "Loss at Epoch  691 :  0.24283786118030548\n",
            "Loss at Epoch  692 :  0.22252963483333588\n",
            "Loss at Epoch  693 :  0.2366374284029007\n",
            "Loss at Epoch  694 :  0.2314993143081665\n",
            "Loss at Epoch  695 :  0.23362436890602112\n",
            "Loss at Epoch  696 :  0.23448218405246735\n",
            "Loss at Epoch  697 :  0.25703194737434387\n",
            "Loss at Epoch  698 :  0.24228283762931824\n",
            "Loss at Epoch  699 :  0.2624603509902954\n",
            "Loss at Epoch  700 :  0.2421979010105133\n",
            "Loss at Epoch  701 :  0.2603931725025177\n",
            "Loss at Epoch  702 :  0.24469631910324097\n",
            "Loss at Epoch  703 :  0.24298112094402313\n",
            "Loss at Epoch  704 :  0.23992495238780975\n",
            "Loss at Epoch  705 :  0.24135029315948486\n",
            "Loss at Epoch  706 :  0.26041749119758606\n",
            "Loss at Epoch  707 :  0.23665092885494232\n",
            "Loss at Epoch  708 :  0.2470874935388565\n",
            "Loss at Epoch  709 :  0.24846616387367249\n",
            "Loss at Epoch  710 :  0.2375253140926361\n",
            "Loss at Epoch  711 :  0.22895625233650208\n",
            "Loss at Epoch  712 :  0.21808560192584991\n",
            "Loss at Epoch  713 :  0.22612687945365906\n",
            "Loss at Epoch  714 :  0.23011814057826996\n",
            "Loss at Epoch  715 :  0.2453947216272354\n",
            "Loss at Epoch  716 :  0.26885586977005005\n",
            "Loss at Epoch  717 :  0.24343463778495789\n",
            "Loss at Epoch  718 :  0.2395016849040985\n",
            "Loss at Epoch  719 :  0.2325003445148468\n",
            "Loss at Epoch  720 :  0.22936943173408508\n",
            "Loss at Epoch  721 :  0.23153766989707947\n",
            "Loss at Epoch  722 :  0.2477298378944397\n",
            "Loss at Epoch  723 :  0.22712939977645874\n",
            "Loss at Epoch  724 :  0.23298154771327972\n",
            "Loss at Epoch  725 :  0.22438760101795197\n",
            "Loss at Epoch  726 :  0.24638327956199646\n",
            "Loss at Epoch  727 :  0.23543010652065277\n",
            "Loss at Epoch  728 :  0.2206919938325882\n",
            "Loss at Epoch  729 :  0.23888082802295685\n",
            "Loss at Epoch  730 :  0.240755096077919\n",
            "Loss at Epoch  731 :  0.2319658398628235\n",
            "Loss at Epoch  732 :  0.2437153309583664\n",
            "Loss at Epoch  733 :  0.23262399435043335\n",
            "Loss at Epoch  734 :  0.24624282121658325\n",
            "Loss at Epoch  735 :  0.24072977900505066\n",
            "Loss at Epoch  736 :  0.24115946888923645\n",
            "Loss at Epoch  737 :  0.23038063943386078\n",
            "Loss at Epoch  738 :  0.2350720763206482\n",
            "Loss at Epoch  739 :  0.24989277124404907\n",
            "Loss at Epoch  740 :  0.2451663315296173\n",
            "Loss at Epoch  741 :  0.24036836624145508\n",
            "Loss at Epoch  742 :  0.24736519157886505\n",
            "Loss at Epoch  743 :  0.2483871877193451\n",
            "Loss at Epoch  744 :  0.24022312462329865\n",
            "Loss at Epoch  745 :  0.23221012949943542\n",
            "Loss at Epoch  746 :  0.22636762261390686\n",
            "Loss at Epoch  747 :  0.2489771693944931\n",
            "Loss at Epoch  748 :  0.24052730202674866\n",
            "Loss at Epoch  749 :  0.23333533108234406\n",
            "Loss at Epoch  750 :  0.24616096913814545\n",
            "Loss at Epoch  751 :  0.23537416756153107\n",
            "Loss at Epoch  752 :  0.2256975620985031\n",
            "Loss at Epoch  753 :  0.2338980883359909\n",
            "Loss at Epoch  754 :  0.24766670167446136\n",
            "Loss at Epoch  755 :  0.22586320340633392\n",
            "Loss at Epoch  756 :  0.24771049618721008\n",
            "Loss at Epoch  757 :  0.2310533970594406\n",
            "Loss at Epoch  758 :  0.2187228798866272\n",
            "Loss at Epoch  759 :  0.23416802287101746\n",
            "Loss at Epoch  760 :  0.2481258362531662\n",
            "Loss at Epoch  761 :  0.236808180809021\n",
            "Loss at Epoch  762 :  0.23410455882549286\n",
            "Loss at Epoch  763 :  0.2447328418493271\n",
            "Loss at Epoch  764 :  0.22267265617847443\n",
            "Loss at Epoch  765 :  0.2266024351119995\n",
            "Loss at Epoch  766 :  0.2348778247833252\n",
            "Loss at Epoch  767 :  0.2338939905166626\n",
            "Loss at Epoch  768 :  0.2319294661283493\n",
            "Loss at Epoch  769 :  0.25371018052101135\n",
            "Loss at Epoch  770 :  0.25456559658050537\n",
            "Loss at Epoch  771 :  0.23993118107318878\n",
            "Loss at Epoch  772 :  0.23344658315181732\n",
            "Loss at Epoch  773 :  0.24646635353565216\n",
            "Loss at Epoch  774 :  0.2561773657798767\n",
            "Loss at Epoch  775 :  0.2259749323129654\n",
            "Loss at Epoch  776 :  0.2338019460439682\n",
            "Loss at Epoch  777 :  0.2304166704416275\n",
            "Loss at Epoch  778 :  0.24288886785507202\n",
            "Loss at Epoch  779 :  0.23212221264839172\n",
            "Loss at Epoch  780 :  0.24384787678718567\n",
            "Loss at Epoch  781 :  0.2402382791042328\n",
            "Loss at Epoch  782 :  0.23019111156463623\n",
            "Loss at Epoch  783 :  0.22906728088855743\n",
            "Loss at Epoch  784 :  0.2280789017677307\n",
            "Loss at Epoch  785 :  0.22404006123542786\n",
            "Loss at Epoch  786 :  0.2414231151342392\n",
            "Loss at Epoch  787 :  0.24297736585140228\n",
            "Loss at Epoch  788 :  0.2621656060218811\n",
            "Loss at Epoch  789 :  0.23628965020179749\n",
            "Loss at Epoch  790 :  0.2606198191642761\n",
            "Loss at Epoch  791 :  0.24369841814041138\n",
            "Loss at Epoch  792 :  0.22296443581581116\n",
            "Loss at Epoch  793 :  0.23596391081809998\n",
            "Loss at Epoch  794 :  0.2298620641231537\n",
            "Loss at Epoch  795 :  0.2315436750650406\n",
            "Loss at Epoch  796 :  0.25789737701416016\n",
            "Loss at Epoch  797 :  0.23514661192893982\n",
            "Loss at Epoch  798 :  0.22819913923740387\n",
            "Loss at Epoch  799 :  0.2346714287996292\n",
            "Loss at Epoch  800 :  0.2480808049440384\n",
            "Loss at Epoch  801 :  0.23164016008377075\n",
            "Loss at Epoch  802 :  0.21899184584617615\n",
            "Loss at Epoch  803 :  0.2532729506492615\n",
            "Loss at Epoch  804 :  0.22544994950294495\n",
            "Loss at Epoch  805 :  0.24153102934360504\n",
            "Loss at Epoch  806 :  0.23662585020065308\n",
            "Loss at Epoch  807 :  0.2268955558538437\n",
            "Loss at Epoch  808 :  0.248856782913208\n",
            "Loss at Epoch  809 :  0.23790647089481354\n",
            "Loss at Epoch  810 :  0.23580527305603027\n",
            "Loss at Epoch  811 :  0.2336767315864563\n",
            "Loss at Epoch  812 :  0.2578545808792114\n",
            "Loss at Epoch  813 :  0.23196116089820862\n",
            "Loss at Epoch  814 :  0.22945742309093475\n",
            "Loss at Epoch  815 :  0.23426547646522522\n",
            "Loss at Epoch  816 :  0.22377189993858337\n",
            "Loss at Epoch  817 :  0.23554177582263947\n",
            "Loss at Epoch  818 :  0.24580810964107513\n",
            "Loss at Epoch  819 :  0.23557192087173462\n",
            "Loss at Epoch  820 :  0.2304752916097641\n",
            "Loss at Epoch  821 :  0.23681247234344482\n",
            "Loss at Epoch  822 :  0.23529931902885437\n",
            "Loss at Epoch  823 :  0.24021483957767487\n",
            "Loss at Epoch  824 :  0.22848789393901825\n",
            "Loss at Epoch  825 :  0.2508634030818939\n",
            "Loss at Epoch  826 :  0.23651161789894104\n",
            "Loss at Epoch  827 :  0.23844805359840393\n",
            "Loss at Epoch  828 :  0.22629985213279724\n",
            "Loss at Epoch  829 :  0.23104062676429749\n",
            "Loss at Epoch  830 :  0.2383524477481842\n",
            "Loss at Epoch  831 :  0.23743486404418945\n",
            "Loss at Epoch  832 :  0.2421083003282547\n",
            "Loss at Epoch  833 :  0.2380562275648117\n",
            "Loss at Epoch  834 :  0.23337651789188385\n",
            "Loss at Epoch  835 :  0.22936958074569702\n",
            "Loss at Epoch  836 :  0.23423530161380768\n",
            "Loss at Epoch  837 :  0.2369323968887329\n",
            "Loss at Epoch  838 :  0.2442186027765274\n",
            "Loss at Epoch  839 :  0.23319396376609802\n",
            "Loss at Epoch  840 :  0.22361530363559723\n",
            "Loss at Epoch  841 :  0.23809580504894257\n",
            "Loss at Epoch  842 :  0.24912860989570618\n",
            "Loss at Epoch  843 :  0.24480034410953522\n",
            "Loss at Epoch  844 :  0.23206090927124023\n",
            "Loss at Epoch  845 :  0.2195621132850647\n",
            "Loss at Epoch  846 :  0.22566835582256317\n",
            "Loss at Epoch  847 :  0.2455051690340042\n",
            "Loss at Epoch  848 :  0.22384430468082428\n",
            "Loss at Epoch  849 :  0.2538553774356842\n",
            "Loss at Epoch  850 :  0.22923004627227783\n",
            "Loss at Epoch  851 :  0.24725189805030823\n",
            "Loss at Epoch  852 :  0.2333575189113617\n",
            "Loss at Epoch  853 :  0.2254701852798462\n",
            "Loss at Epoch  854 :  0.23047050833702087\n",
            "Loss at Epoch  855 :  0.23963864147663116\n",
            "Loss at Epoch  856 :  0.23292584717273712\n",
            "Loss at Epoch  857 :  0.21409733593463898\n",
            "Loss at Epoch  858 :  0.24803954362869263\n",
            "Loss at Epoch  859 :  0.2293318808078766\n",
            "Loss at Epoch  860 :  0.24170136451721191\n",
            "Loss at Epoch  861 :  0.23688490688800812\n",
            "Loss at Epoch  862 :  0.2432396411895752\n",
            "Loss at Epoch  863 :  0.21826386451721191\n",
            "Loss at Epoch  864 :  0.247803196310997\n",
            "Loss at Epoch  865 :  0.21391284465789795\n",
            "Loss at Epoch  866 :  0.2515611946582794\n",
            "Loss at Epoch  867 :  0.248411625623703\n",
            "Loss at Epoch  868 :  0.2344110906124115\n",
            "Loss at Epoch  869 :  0.22450695931911469\n",
            "Loss at Epoch  870 :  0.23840072751045227\n",
            "Loss at Epoch  871 :  0.2477589100599289\n",
            "Loss at Epoch  872 :  0.25459688901901245\n",
            "Loss at Epoch  873 :  0.23449614644050598\n",
            "Loss at Epoch  874 :  0.23445561528205872\n",
            "Loss at Epoch  875 :  0.22927631437778473\n",
            "Loss at Epoch  876 :  0.2379637509584427\n",
            "Loss at Epoch  877 :  0.25540801882743835\n",
            "Loss at Epoch  878 :  0.2281394749879837\n",
            "Loss at Epoch  879 :  0.2286817580461502\n",
            "Loss at Epoch  880 :  0.22243034839630127\n",
            "Loss at Epoch  881 :  0.23244856297969818\n",
            "Loss at Epoch  882 :  0.2356005609035492\n",
            "Loss at Epoch  883 :  0.2403373271226883\n",
            "Loss at Epoch  884 :  0.2500959038734436\n",
            "Loss at Epoch  885 :  0.23374289274215698\n",
            "Loss at Epoch  886 :  0.24726298451423645\n",
            "Loss at Epoch  887 :  0.23398898541927338\n",
            "Loss at Epoch  888 :  0.22709636390209198\n",
            "Loss at Epoch  889 :  0.21680107712745667\n",
            "Loss at Epoch  890 :  0.2317139357328415\n",
            "Loss at Epoch  891 :  0.22484777867794037\n",
            "Loss at Epoch  892 :  0.24375422298908234\n",
            "Loss at Epoch  893 :  0.23706486821174622\n",
            "Loss at Epoch  894 :  0.22798573970794678\n",
            "Loss at Epoch  895 :  0.2400226593017578\n",
            "Loss at Epoch  896 :  0.23779961466789246\n",
            "Loss at Epoch  897 :  0.21888060867786407\n",
            "Loss at Epoch  898 :  0.25806328654289246\n",
            "Loss at Epoch  899 :  0.22861173748970032\n",
            "Loss at Epoch  900 :  0.24378573894500732\n",
            "Loss at Epoch  901 :  0.22248633205890656\n",
            "Loss at Epoch  902 :  0.22824284434318542\n",
            "Loss at Epoch  903 :  0.21649526059627533\n",
            "Loss at Epoch  904 :  0.24475471675395966\n",
            "Loss at Epoch  905 :  0.22993247210979462\n",
            "Loss at Epoch  906 :  0.22231550514698029\n",
            "Loss at Epoch  907 :  0.25048768520355225\n",
            "Loss at Epoch  908 :  0.2365526556968689\n",
            "Loss at Epoch  909 :  0.24596789479255676\n",
            "Loss at Epoch  910 :  0.23319369554519653\n",
            "Loss at Epoch  911 :  0.25596100091934204\n",
            "Loss at Epoch  912 :  0.2302008718252182\n",
            "Loss at Epoch  913 :  0.22499588131904602\n",
            "Loss at Epoch  914 :  0.23432520031929016\n",
            "Loss at Epoch  915 :  0.23590852320194244\n",
            "Loss at Epoch  916 :  0.23357711732387543\n",
            "Loss at Epoch  917 :  0.24088837206363678\n",
            "Loss at Epoch  918 :  0.23179534077644348\n",
            "Loss at Epoch  919 :  0.22394433617591858\n",
            "Loss at Epoch  920 :  0.22505110502243042\n",
            "Loss at Epoch  921 :  0.21648794412612915\n",
            "Loss at Epoch  922 :  0.22737297415733337\n",
            "Loss at Epoch  923 :  0.2531891465187073\n",
            "Loss at Epoch  924 :  0.22123485803604126\n",
            "Loss at Epoch  925 :  0.22993580996990204\n",
            "Loss at Epoch  926 :  0.23229289054870605\n",
            "Loss at Epoch  927 :  0.2256731241941452\n",
            "Loss at Epoch  928 :  0.23361431062221527\n",
            "Loss at Epoch  929 :  0.22284065186977386\n",
            "Loss at Epoch  930 :  0.21898561716079712\n",
            "Loss at Epoch  931 :  0.2267504632472992\n",
            "Loss at Epoch  932 :  0.23574711382389069\n",
            "Loss at Epoch  933 :  0.24100174009799957\n",
            "Loss at Epoch  934 :  0.2248670607805252\n",
            "Loss at Epoch  935 :  0.23640798032283783\n",
            "Loss at Epoch  936 :  0.23178191483020782\n",
            "Loss at Epoch  937 :  0.23769602179527283\n",
            "Loss at Epoch  938 :  0.23538050055503845\n",
            "Loss at Epoch  939 :  0.2432274967432022\n",
            "Loss at Epoch  940 :  0.23667478561401367\n",
            "Loss at Epoch  941 :  0.24204480648040771\n",
            "Loss at Epoch  942 :  0.23582759499549866\n",
            "Loss at Epoch  943 :  0.23301827907562256\n",
            "Loss at Epoch  944 :  0.24017736315727234\n",
            "Loss at Epoch  945 :  0.2314758598804474\n",
            "Loss at Epoch  946 :  0.2225552499294281\n",
            "Loss at Epoch  947 :  0.2433784157037735\n",
            "Loss at Epoch  948 :  0.23616842925548553\n",
            "Loss at Epoch  949 :  0.22423669695854187\n",
            "Loss at Epoch  950 :  0.23060864210128784\n",
            "Loss at Epoch  951 :  0.23200540244579315\n",
            "Loss at Epoch  952 :  0.2344609647989273\n",
            "Loss at Epoch  953 :  0.22806237637996674\n",
            "Loss at Epoch  954 :  0.24204520881175995\n",
            "Loss at Epoch  955 :  0.2214556336402893\n",
            "Loss at Epoch  956 :  0.24832358956336975\n",
            "Loss at Epoch  957 :  0.23194028437137604\n",
            "Loss at Epoch  958 :  0.2404516637325287\n",
            "Loss at Epoch  959 :  0.23971325159072876\n",
            "Loss at Epoch  960 :  0.240011066198349\n",
            "Loss at Epoch  961 :  0.22913771867752075\n",
            "Loss at Epoch  962 :  0.2196349799633026\n",
            "Loss at Epoch  963 :  0.23923583328723907\n",
            "Loss at Epoch  964 :  0.24392929673194885\n",
            "Loss at Epoch  965 :  0.21867145597934723\n",
            "Loss at Epoch  966 :  0.23113428056240082\n",
            "Loss at Epoch  967 :  0.2334146946668625\n",
            "Loss at Epoch  968 :  0.22364702820777893\n",
            "Loss at Epoch  969 :  0.2132277637720108\n",
            "Loss at Epoch  970 :  0.2437649369239807\n",
            "Loss at Epoch  971 :  0.23187845945358276\n",
            "Loss at Epoch  972 :  0.22729499638080597\n",
            "Loss at Epoch  973 :  0.23961982131004333\n",
            "Loss at Epoch  974 :  0.2398678958415985\n",
            "Loss at Epoch  975 :  0.21883586049079895\n",
            "Loss at Epoch  976 :  0.23797453939914703\n",
            "Loss at Epoch  977 :  0.2275656759738922\n",
            "Loss at Epoch  978 :  0.21851958334445953\n",
            "Loss at Epoch  979 :  0.2142200469970703\n",
            "Loss at Epoch  980 :  0.24962063133716583\n",
            "Loss at Epoch  981 :  0.23801667988300323\n",
            "Loss at Epoch  982 :  0.23090197145938873\n",
            "Loss at Epoch  983 :  0.23781540989875793\n",
            "Loss at Epoch  984 :  0.22824540734291077\n",
            "Loss at Epoch  985 :  0.22508320212364197\n",
            "Loss at Epoch  986 :  0.23243755102157593\n",
            "Loss at Epoch  987 :  0.2376093715429306\n",
            "Loss at Epoch  988 :  0.22519119083881378\n",
            "Loss at Epoch  989 :  0.23503200709819794\n",
            "Loss at Epoch  990 :  0.2389855533838272\n",
            "Loss at Epoch  991 :  0.25291842222213745\n",
            "Loss at Epoch  992 :  0.23980900645256042\n",
            "Loss at Epoch  993 :  0.22414293885231018\n",
            "Loss at Epoch  994 :  0.2266860157251358\n",
            "Loss at Epoch  995 :  0.23166730999946594\n",
            "Loss at Epoch  996 :  0.2361140102148056\n",
            "Loss at Epoch  997 :  0.23343273997306824\n",
            "Loss at Epoch  998 :  0.24044789373874664\n",
            "Loss at Epoch  999 :  0.24056005477905273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we compute the accuracy of our model. This is copied from P8."
      ],
      "metadata": {
        "id": "2mnra8Q42-eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_classification_model(test_loader, model):\n",
        "    \"\"\"Tests the accuracy of the model.\n",
        "\n",
        "    Input:\n",
        "        test_loader:      Data loader for the test set.\n",
        "                          Enumerate through to test each example.\n",
        "        model:            nn.Module model being evaluate.\n",
        "\n",
        "    Output:\n",
        "        accuracy:         Accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "    # Compute the model accuracy\n",
        "    total_batches = 0\n",
        "    classified_batches = 0\n",
        "\n",
        "    for index, (batch, labels) in enumerate(test_loader):\n",
        "        preds = ((model(batch) >= 0.5) * 1).view(len(labels))\n",
        "        total_batches += len(labels)\n",
        "\n",
        "        print(preds)\n",
        "        print(labels)\n",
        "        print(preds == labels)\n",
        "\n",
        "        classified_batches += torch.sum(preds == labels).item()\n",
        "\n",
        "    return classified_batches / total_batches\n",
        "\n",
        "print(test_classification_model(loader, lstm_model))"
      ],
      "metadata": {
        "id": "makRyFhetWIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155675ea-c25a-4967-acd5-75f2e32fe962"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0, 0,  ..., 1, 0, 1])\n",
            "tensor([1., 0., 0.,  ..., 1., 0., 1.])\n",
            "tensor([True, True, True,  ..., True, True, True])\n",
            "tensor([1, 1, 1,  ..., 0, 0, 1])\n",
            "tensor([1., 1., 1.,  ..., 0., 0., 1.])\n",
            "tensor([True, True, True,  ..., True, True, True])\n",
            "tensor([0, 1, 0,  ..., 0, 0, 0])\n",
            "tensor([1., 1., 0.,  ..., 0., 0., 1.])\n",
            "tensor([False,  True,  True,  ...,  True,  True, False])\n",
            "tensor([1, 1, 0,  ..., 1, 1, 0])\n",
            "tensor([1., 1., 0.,  ..., 1., 1., 0.])\n",
            "tensor([True, True, True,  ..., True, True, True])\n",
            "tensor([0, 0, 1,  ..., 1, 0, 1])\n",
            "tensor([0., 0., 1.,  ..., 1., 0., 1.])\n",
            "tensor([True, True, True,  ..., True, True, True])\n",
            "tensor([0, 0, 1,  ..., 0, 1, 1])\n",
            "tensor([0., 0., 1.,  ..., 0., 1., 1.])\n",
            "tensor([True, True, True,  ..., True, True, True])\n",
            "0.89488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it's time to see what our predictions are for the test data, which we save in a CSV file with the corresponding UID's and predictions."
      ],
      "metadata": {
        "id": "x8xMkj1Y3CFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "test_emb1_tensor = torch.Tensor(test_emb1)\n",
        "test_emb2_tensor = torch.Tensor(test_emb2)\n",
        "test_emb_combined = torch.cat((test_emb1_tensor, test_emb2_tensor), 1)\n",
        "test_emb_combined_normalized = F.normalize(test_emb_combined)\n",
        "\n",
        "uids = np.arange(18750,25000,1)\n",
        "print(uids.shape)\n",
        "\n",
        "test_predictions = ((lstm_model(test_emb_combined_normalized) >= 0.5) * 1).detach().numpy().reshape(-1)\n",
        "print(test_predictions.shape)\n",
        "\n",
        "df = pd.DataFrame({ 'uid' : uids,\n",
        "    'preference' : test_predictions})\n",
        "print(df)\n",
        "\n",
        "now = datetime.now()\n",
        "df.to_csv('/content/drive/My Drive/mydata' + now.strftime(\"%H:%M:%S\") + '.csv', index=False)"
      ],
      "metadata": {
        "id": "gGs4KngJ66uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae81c56b-1d39-4092-e0f2-cdbac399c283"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6250,)\n",
            "(6250,)\n",
            "        uid  preference\n",
            "0     18750           0\n",
            "1     18751           0\n",
            "2     18752           1\n",
            "3     18753           0\n",
            "4     18754           1\n",
            "...     ...         ...\n",
            "6245  24995           1\n",
            "6246  24996           0\n",
            "6247  24997           1\n",
            "6248  24998           0\n",
            "6249  24999           0\n",
            "\n",
            "[6250 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}